{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) and Deep Architectures\n",
    "\n",
    "In these practicals, we will create a first convolutional neural network (CNN). This type of networks mostly rely on alternating layers of convolution and pooling. A convolution layer is a signal processing inspired layer that performs a bunch of convolutions on the layer input and yields filtered outputs known as feature maps. A pooling layer is a sort of subsampling layer. Most of the time, after several Conv/Pool layer pairs, an MLP is plugged to obtain class membership probabilities. When the number of layers is rather high (unclear how high this is), we obtain a deep network.\n",
    "\n",
    "We will use `TensorFlow v2.O` (or higher) as programming framework and build a CNN to solve an image classification task.\n",
    "\n",
    "## Starting with TensorFlow\n",
    "\n",
    "`TensorFlow` is Google's library designed to create and train deep nets. It relies on an underlying computation graph. Each node of this graph performs an operation on an input tensor. The possible number of ops is very large (ranging from addition, dot product and more complex non-linear operations). The input tensor of the network will flow through these nodes, hence the name of the library. \n",
    "\n",
    "Here is an example of computation graph:\n",
    "\n",
    "<img src=\"graph.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "This graph is a tree whose final node output correponds to the following function :\n",
    "\n",
    "$$f\\left( x,y\\right) = x^2 + 2y,$$\n",
    "\n",
    "for some $x$ and $y$. \n",
    "To implement this tree in `TensorFlow`, each node will be an instance of the `Tensor` class. \n",
    "Let us start with the inputs of function $f$ and suppose, we are interested in evaluation function $f$ for the following three pairs $(x,y)$:\n",
    "* $(x,y) = (1,2)$\n",
    "* $(x,y) = (2,3)$\n",
    "* $(x,y) = (3,4)$\n",
    "\n",
    "Starting with $x$, we shall define a tensor containing the successive values of this input : \n",
    "```python\n",
    "x = tf.constant([1,2,3])\n",
    "type(x)\n",
    "```\n",
    "The console answers that `x` has type `<class 'tensorflow.python.framework.ops.EagerTensor'>`. Let us forget about the \"eager\" version and focus on the fact `x`is indeed a tensor ! \n",
    "\n",
    "Now we do the same with $y$:\n",
    "```python\n",
    "y = tf.constant([2,3,4])\n",
    "```\n",
    "We don't have to define a python variable for each node of the computation graph. We can now ask `TensorFlow` to build the whole graph by typing:\n",
    "\n",
    "```python\n",
    "f = x*x + 2*y\n",
    "print(f)\n",
    "```\n",
    "The console answers: `tf.Tensor([ 5 10 17], shape=(3,), dtype=int32)`. \n",
    "Observe that `f` is a tensor that contains the three desired evaluations of function $f$. If one is only interested in observing the array of values contained in `f`, one can ask for the corresponding `numpy` array:\n",
    "\n",
    "```python\n",
    "print(f.numpy())\n",
    "```\n",
    "\n",
    "Before moving to our first exercise, let us remark that instances of `Tensor` are immutable objects ! **They can be assigned only once**. These objects are not meant to vary accross training. For instance, if one types:\n",
    "\n",
    "```python\n",
    "x[0]=10\n",
    "```\n",
    "The console answers : `TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment`.\n",
    "\n",
    "In the next practical session, we will use other tensorflow objects that accept multiple assignments: `tf.Variable`.\n",
    "\n",
    "Here is a cell containing the above instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor([ 5 10 17], shape=(3,), dtype=int32)\n",
      "[ 5 10 17]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-21198cc398d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([1,2,3])\n",
    "print(type(x))\n",
    "y = tf.constant([2,3,4])\n",
    "f = x*x + 2*y\n",
    "print(f)\n",
    "print(f.numpy())\n",
    "x[0]=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN\n",
    "\n",
    "We are now almost ready to start building a `TensorFlow` graph containing convolution or pooling layers instead of the basic operations involved in function $f$. \n",
    "But first things first ! We need a dataset to work with and to load in Jupyter. We will be working with the `MNIST` dataset. This dataset contains 28x28 grayscale images of handwritten figures. It has the advantage to be larger than traditional datasets utilized in practicals while it can still hold in memory and converge in a few minutes. Larger or more complicated datasets (more variability in images) would require too many computational efforts.\n",
    "\n",
    "Execute the following cell to load `MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uty0Adev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpHPQKowSG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7rsE0CXJhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7EmHAGrRNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTSUi1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7i7VgF0o+1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbt6t55/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255 - 0.5\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test/255 - 0.5\n",
    "x_test = x_test.astype('float32')\n",
    "#\n",
    "print(y_train[0])\n",
    "plt.imshow(x_train[0], cmap='gray', interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some constants that we be useful in the sequel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "d_x = x_train.shape[1]\n",
    "print(x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin to build the CNN. Following the explanations in the preamble, it seems like we need to turn the arrays of the dataset into `Tensor` instances. That won't be explicitely necessary because `TensorFlow` will do that on its own. So we can readily define a firt convoluational layer that we will connect to the array `x_train`. \n",
    "\n",
    "A second point to mention is that we will use high level functions in this practical session, i.e. functions that in reality translate in multiple nodes of the graph. These functions and classes belong to the `Keras` API. It is accessiable through `tf.Keras`. Note that the backend of this API is `TensorFlow` itself !! So using `Keras` somewhat shadows the use of the computation graph. However, we will see that building a model with `Keras` follows the same philosophy of piling up operations.\n",
    "\n",
    "To use `Keras` functions, we need to prepare ourselves a little bit:\n",
    "* by reshaping our input tensor as 4D tensor where the 4th and last dimension is 1. Later in the graph, this dimension will be larger because we will use multiple filters,\n",
    "* by creating an instance from `tf.keras.Input` which is compatible with `x_train` and plays the role of \"input layer\" for a `Keras` model even if it does not contain actual data\n",
    "\n",
    "### Q1\n",
    "* Reshape `x_train` by adding a 4th dimension of size 1 to it. This dimension is referred to as \"image channel\".\n",
    "* Fill the gap in the instantiation of `tf.keras.Input`. The `shape` parameter must receive a tuple containing image height, image width and image channels. This means that the dataset size is not passed to the constructor because `Keras` does not need this information to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train,(n,d_x,d_x,1))\n",
    "inputs = tf.keras.Input(shape=x_train.shape[1:])\n",
    "print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next element of our NN is a 2D convolutional layer. We can use an instance of `tf.keras.layers.Conv2D` class. We need to specify the number of size of filters (kernels) that we want, what filter size, how to deal with image borders (padding) and what activation we would like. \n",
    "\n",
    "This layer is plugged to the input layer by passing it as input, as if `conv1` was also a function. This way of connecting `Keras` layers will be the same in the sequel.\n",
    "\n",
    "This gives:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer #1\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a pooling layer to give up some entries of the filtered images provided by the convolutional layer. We will use the usual max pool function in 2x2 neighborhoods which drops 3 entries out of 4 in each such neighborhood. The remaining entry is the one achieving maximal value in the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling Layer #1\n",
    "pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "* Why is `strides=2` the correct parameter value for our goal ? \n",
    "* What is the size of each feature map after max pooling ?\n",
    "* How many feature maps are there ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We use strides=2 because we want to decrease the sizes of feature maps.\n",
    "\n",
    "-In MaxPooling input, we have this size from the previous convolution layer : (24,24) (28-5+1), with MaxPooling, we divide feature maps size by 2 so we obtain (12,12).\n",
    "\n",
    "-As in the previous layer we have chosen 64 as the number of filter, we have 64 feature maps before MaxPooling, after MaxPooling, we still have 64 feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now plug a 1-layer-MLP on top of this. Fully connected layers (or dense layers) do not accept 4D tensors but 2D ones. We can use `tf.reshape` to flatten feature maps / images to vectors. In the following code the scalar `dflat` = nbr of feature maps x feature map width x feature map height. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q3\n",
    "* Set this variable with the correct value and execute the following code line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat = pool1.shape[1]*pool1.shape[2]*pool1.shape[3]\n",
    "pool2_flat = tf.reshape(pool1, [-1, dflat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the first layer MLP is given by an instance of `tf.keras.layers.Dense` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)(pool2_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a softmax layer to obtain class membership probabilities. This can be done in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.keras.layers.Dense(units=10)(mlp)\n",
    "smax = tf.keras.layers.Softmax()(logits)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=smax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we need to compile the `Keras` model. During this operation, we give the desired values for remaining hyperparameters. We also specify a loss, which in our case is cross-entropy. \n",
    "\n",
    "After compiling, `Keras` allows to have an overview of the model by invoking the `summary` function. This shows us how many parameters there are per layer. \n",
    "\n",
    "### Q4\n",
    "* Explain these numbers layer per layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 24, 24, 64)        1664      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              9438208   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,450,122\n",
      "Trainable params: 9,450,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of greatest feature of `Keras` is that the training loop is directly implemented through the `fit` function of our model. Let us start training our model for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 255/3000 [=>............................] - ETA: 1:59 - loss: 0.0210 - categorical_accuracy: 0.9924"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-bd9f5d56d650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    647\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m     \"\"\"\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    690\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    691\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    467\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    470\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0;32m    471\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train, y=tf.keras.utils.to_categorical(y_train), batch_size=20, epochs=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that we did not overfit, we need to evaluate our model on the test set.\n",
    "\n",
    "## Q5\n",
    "* reshape `x_test` in a 4D tensor just like we did for `x_train`\n",
    "* document yourself on the evaluate function of `tf.Keras.Model` and compute the model accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0320 - categorical_accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "n_test=x_test.shape[0]\n",
    "x_test = np.reshape(x_test,(n_test,d_x,d_x,1))\n",
    "res = model.evaluate(x=x_test,y=tf.keras.utils.to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "* Add a second pair of conv/pool layers. A smaller amount amount of filters and smaller filters are recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        1664      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 9, 32)          32800     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_2 (TFOpLambda)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      " softmax_2 (Softmax)         (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 570,026\n",
      "Trainable params: 570,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.keras.layers.Conv2D(\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)(inputs)\n",
    "\n",
    "pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv1)\n",
    "\n",
    "conv2 = tf.keras.layers.Conv2D(\n",
    "      filters=32,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)(pool1)\n",
    "pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv2)\n",
    "\n",
    "dflat_2 = pool2.shape[1]*pool2.shape[2]*pool2.shape[3]\n",
    "pool2_flat = tf.reshape(pool2, [-1, dflat_2])\n",
    "\n",
    "mlp = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)(pool2_flat)\n",
    "\n",
    "logits = tf.keras.layers.Dense(units=10)(mlp)\n",
    "smax = tf.keras.layers.Softmax()(logits)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=smax)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3000/3000 [==============================] - 39s 13ms/step - loss: 0.1022 - categorical_accuracy: 0.9685\n",
      "Epoch 2/2\n",
      "3000/3000 [==============================] - 39s 13ms/step - loss: 0.0413 - categorical_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256121c9eb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=tf.keras.utils.to_categorical(y_train), batch_size=20, epochs=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0371 - categorical_accuracy: 0.9891\n"
     ]
    }
   ],
   "source": [
    "n_test=x_test.shape[0]\n",
    "x_test = np.reshape(x_test,(n_test,d_x,d_x,1))\n",
    "res = model.evaluate(x=x_test,y=tf.keras.utils.to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q7\n",
    "* Add a second fully connected layer. A smaller number of units is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 64)        1664      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 9, 9, 32)          32800     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " tf.reshape_3 (TFOpLambda)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,089,706\n",
      "Trainable params: 1,089,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.keras.layers.Conv2D(\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)(inputs)\n",
    "\n",
    "pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv1)\n",
    "\n",
    "conv2 = tf.keras.layers.Conv2D(\n",
    "      filters=32,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)(pool1)\n",
    "pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv2)\n",
    "\n",
    "dflat_2 = pool2.shape[1]*pool2.shape[2]*pool2.shape[3]\n",
    "pool2_flat = tf.reshape(pool2, [-1, dflat_2])\n",
    "\n",
    "mlp1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)(pool2_flat)\n",
    "\n",
    "mlp2 = tf.keras.layers.Dense(units=512, activation=tf.nn.relu)(mlp1)\n",
    "\n",
    "logits = tf.keras.layers.Dense(units=10)(mlp2)\n",
    "smax = tf.keras.layers.Softmax()(logits)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=smax)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3000/3000 [==============================] - 46s 15ms/step - loss: 0.1158 - categorical_accuracy: 0.9649\n",
      "Epoch 2/2\n",
      "3000/3000 [==============================] - 46s 15ms/step - loss: 0.0499 - categorical_accuracy: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256128f3e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=tf.keras.utils.to_categorical(y_train), batch_size=20, epochs=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0391 - categorical_accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "n_test=x_test.shape[0]\n",
    "x_test = np.reshape(x_test,(n_test,d_x,d_x,1))\n",
    "res = model.evaluate(x=x_test,y=tf.keras.utils.to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q8 (optional)\n",
    "From the above NN architecture, try to tweak the other hyperparameters and comment on the sensitivity of the results wrt them. These latter include:\n",
    "* the learning rate,\n",
    "* the regularization strength (if regularization is activated),\n",
    "* number of filters,\n",
    "* filter sizes,\n",
    "* activiation functions,\n",
    "* strides,\n",
    "* padding types,\n",
    "* ...\n",
    "\n",
    "### Q9\n",
    "* Compare the obtained accuracy to that of a regularized Softmax regression from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
